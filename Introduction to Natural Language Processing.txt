Introduction to Natural Language Processing
===========================================

Week 1
======
Introduction 1/2

NLP is the study of the computational treatment of natural (human) language.

Computers to understand (and generate) human language.

Modern application:
Search engines
Question Answering (IBM's Watson)
Natural language assistants (Siri)
Translation Systems (Google Translate)
News digest (Yahoo)

NLP draws on research in
a. Linguistics
b. Theoretical Computer Science
c. Mathematics
d. Statistics
e. AI
f. Psychology
g. Databases, etc.

Goals of this class:
a. Understand that language processing is hard.
b. Understand they key problems in NLP.
c. Learn about the methods used to address these problems.
d. Understand the limitations of these methods.

Language and Communication:
a. Speaker
- Intention (goal, shared knowledge, and beliefs)
- Generation (tactical)
- Synthesis (text or speech)

b. Listener
- Perception
- Interpretation (syntactic, semantic, pragmatic)
- Incorporation (internalization, understanding)

c. Both
- Context (grounding)

Basic NLP Pipeline:
(U)ndestanding & (G)eneration
Language ---- (U) ---> Computer --- (G) ----> Language

Genres of Text:
Blogs, emails, press releases, chat, debates, etc
Scientific research papers, fiction books, Poetry

Ambiguous nature of sentences results in NLP hard problems.
a. Lexical Ambiguity
b. Structural Ambiguity
c. Scope Ambiguity

Structure of the course:
a. Four major parts.
	i. Linguistic, mathematical, and computational background.
	ii. Computational models of morphology, syntax, semantics, discourse, pragmatics.
	iii. Core NLP technology: parsing, POS tagging, text generation, etc.
	iv. Applications: text classification, machine translation, information extraction, etc

b. Three major goals
	i. Learn the basic principles and theoretical issues underlying NLP.
	ii. Learn techniques and tools used to develop practical, robust systems.
	iii. Gain insight into some open research problems in NLP.

Why NLP is hard?
Metaphorical sentences are hard for NLP
For e.g Time flies like an arrow.

Common sense is hard for NLP machines
For e.g Each American has a mother. (Multiple mothers)
Each American has a president. (Single president)

Syntax error
* Little a has Mary lamb.

Semantic error
? Colorless green ideas sleep furiously.

Ambigous words.

Others ambiguity:
Morphological ambiguity (Structure of a sentence)
Phonetic ambiguity
Part of Speech
Syntactic
PP attachment
Sense
Modality
Subjectivity
CC attachment
Negation
Referential
Reflexive
Ellipsis and parallelism
Metonympy (the substitution of the name of an attribute or adjunct for that of the thing meant)

Other Sources of Difficulties:
Non-standard, slang, and novel words
Complex sentences
Humor and sarcasm
Absense of world knowledge
Semantics and pragmatics

Synonyms and Paraphrases

Most of the ambiquity are fairly common sense, which computers doesn't posses.

Linguistic knowledge:
a. Constituents
For e.g Children eat pizza or They eat pizza.

b. Collocations
For e.g red wine, but not brown wine

How to get this knowledge in the system:
- Manual rules
- Automatic acquire from large text collections (corpora)

Knowledge about language:
a. Phonetics and phonology: the study of sound
b. Morphology: the study of word components
c. Syntax: the study of sentence and phrase structure
d. Lexical semantics: the study of meanings of words
e. Compositional semantics: how to combine words
f. Pragmatics: how to accomplish goals
g. Discourse conventions: how to deal with units larger than utterances

How to store language knowledge:
Finite-state automata
Grammars
Complexity
Dynamic Programming

Mathematics and Statistics:
Probability
Statistical Models
Hypothesis testing
Linear algebra
Optimization
Numerical methods

Mathematical and Computational tools:
Language models
Estimation methods
Context-free Grammar (CFG)
Hidden Markov Models (HMM)
Conditional Random Fields (CRF)
Generative/discriminative models
Maximum entropy models

Statistical Techniques:
Vector space representation for WSD
Noisy channel models for MT
Graph-based Random walk methods for sentiment analysis

Aritficial Intelligence:
Logic
Agents
Planning
Constraint satisfaction
Machine learning

Linguistics:
IPA Chart for language pronouncation
Languages are related, all derived from Proto-Indo-European Langauge (PIE)

Language changes over time.

Diversity of languages:
Absence of Articles
Cases
Sound systems
Social status

NACLO Problems

Language Universals:
Unconditional and conditional types of universals; common in all languages.

==================================================================================

Week 2
======
Introduction 2/2

Part of Speech:
Noun, pronoun, articles, adjectives, adverb, etc.

Computers see text that they don't really understand.
They have to use some prior knowledge.
They reason probabilistically.
They use context.
They can be wrong.

Morphology and the Lexicon:

In linguistics, morphology is the study of words, how they are formed, and their relationship to other words in the same language. It analyzes the structure of words and parts of words, such as stems, root words, prefixes, and suffixes.

Mental Lexicon consits of meaning of word, its pronounciation, part of speech.
Without these, morphology of words are very important.
Intuition and productivity also plays a vital role.

Morphological Analysis:
sleeps = sleep + V + 3P + SG
done = do + V + PP

Semantics:
Lexical and compositional semantics
Lexical is meaning of individual words, and compositional is understanding the meaning of sentence based on meaning of components.

Pragmatics:
The study of how knowledge about the world and language conventions interact with literal meaning.

Text Similarity:
Similarity dataset consits of two words and a similarity score given by human judgements.
Word2vec model is used for text similarity.

Many types of similarity:
Morphological
Spelling
Synonymy
Homophony
Semantic
Sentence (Paraphrases)
Document
Cross-lingual

Morphological similarity: Stemming:
Word with the same root.

To stem a word is to reduce it to a base form, called the stem, after removing various suffixes and endings and, sometimes, performing some additional transformations.

Porter's stemming method is a rule based algorithm.
The method is not always accurate.
The measure of a word is an indication of the number of syllables in it.
Porter's algorithm use measure of a word to convert it into stem using various steps.

Spelling Similarity: Edit Distance:
It can be achieved using Edit operations.

Levenshtein Method:
Based on Dynamic Programming
Insertion, deletion, and substitutions have cost 1.

Recursive Relation:
s1(i) - ith char in string s1
s2(j) - jth char in string s2
D(i, j) - edit distance between a prefix of s1 of length i and prefix of s2 of length j
t(i, j) - cost of aligning the ith character in string s1 with jth character in string s2

Recursive dependencies:
D(i, 0) = i
D(0, j) = j
D(i, j) = min (D(i-1, j) + 1, D(i, j-1) + 1, D(i-1, j-1) + t(i, j))

t(i, j) = 0 if s1(i) = s2(j)
t(i, j) = 1 otherwise

Other costs:
Damerau modification

Other costs includes keyboard typo and OCR mistakes.

NACLO:
Competition in Linguistics

Preprocessing:
Removing non-text
Dealing with text encoding
Sentence segmentation
Normalisation
Stemming
Morphological analysis
Capitalization
Name-entity extraction

Types or Tokens
Tokenization

Word segmenation in many language are very difficult.

Sentence boundary Recognition:
Decision trees
Features

=====================================================================================================

Week 3
======
NLP Tasks and Text Similarity

Semantic Similarity: Synonyms and Paraphrases
Synonyms: Different words having similar meaning.

Polysemy: is the property of words to have multiple senses. They can be used as multiple POS.

Synsets: Group together all synonyms of the same word.

WordNet dataset:
Database of words and semantic relations between them.
Tree structure of different words.

Other dataset:
EuroWordNet
Open Thesaurus
Freebase
DBPedia
BabelNet

MeSH - Medical Subject Headings

Thesaurus-based Word Similarity Methods:
Path Similarity
Version 1:
Sim(v, w) = - pathlength(v, w)

Version 2:
Sim(v, w) = - log pathlength(v, w)

Problems:
There may be no tree for the specific domain or language.
A specific word may not be in any tree.
Is-A edges are not all equally apart in similarity space.

Version 3:
Sim(v, w) = - log P(LCS(v, w))
where LCS = lowest common subsumer

Least Common Subsumer of two concepts A and B is "the most specific concept which is an ancestor of both A and B", where the concept tree is defined by the is-a relation.

Version 4: (Lin Similarity)
IC(c) = - log P(c) # IC - Information Content
Sim(v, w) = 2 * log P(LCS(v, w)) / (log P(v) + log P(w))

Implemented in Python: nltk
Usage: dog.lin_similarity(cat, brown_ic)

The Vector Space model:
Used in information retrieval to determine which document is more similar to a given query q.
Documents and queries are representated in the same space.
Often, the angle between two vectors is used as a proxy for the similarity of the underlying documents.

Cosine Similarity:
Normalised dot products of two vectors.
Sim(d, q) = Sigma (di.qi) / (sqrt(Sigma (di)^2)) (sqrt(Sigma (qi)^2)) 

or Jaccard coefficient
Sim(d, q) = |d intersection q| / |d union q|

Distributional Similarity:
Two words that appear in similar contexts are likely to be semantically related.
However, the context can be any of the following:
a. The word before the target word.
b. The word after the target word.
c. Any word within n words of the target words.
d. Any word within a specific syntactic relationship with target word.
e. Any word within the same sentence.
f. Any word within the same document.

Dimensionality Reduction:

Problems with the simple vector approaches to similarity:
a. Polysemy (sim < cos)
b. Synonymy (sim > cos)
c. Relatedness
d. Sparse matrix

Eigen vectors for matrix reduction in case of square matrices.

Singular Value Decomposition (SVD) in other cases.

Latent Semantic Indexing (LSI):
Dimensionality reduction = identification of hidden (latent concepts)
Query matching in latent space.

NLP Tasks:
Part of Speech taggging
Parsing
Dependency Parsing
Information Extraction
Semantic analysis (First order logic, Inference)
Reading comprehension
Text Understanding
Word Sense disambiguation
Named Entity Recognition
Semantic Role Labeling
Coreference Resolution
Question Answering
Sentiment Analysis
Machine Translation
Text Summarization
Text to Speech
Entailment and Paraphrasing
Dialogue System, and etc.

Garden path sentences:
For e.g
Don't bother coming
Don't bother coming early.

=========================================================================================

Week 4
======
Syntax and Parsing Part 1

Syntax:
Grammatical rules apply to categories and groups of words, not individual words.

Constituents:
Continous, non-crossing words.

Constituents test:
a. "coordination" test
b. "pronoun" test
c. "question by repetition" test
d. "topicalization" test
e. "question" test, and etc

How to generate sentences:
One way: tree structure using grammar (CFG) rules

Adjective ordering:
det < number < size < color < purpose < noun

Recursion:
Allows to generate long sentences.

Parsing:
Associating tree structures to a sentence, given a grammar (often CFG)

Application of Parsing:
a. Grammar checking
b. Question answering
c. Machine translation
d. Information extraction
e. Speech generation, and etc

Context-free Grammar:
4-tuple (N, Sigma, R, S), where
N = non-terminal symbols.
Sigma = terminal symbols.
R = set of rules alpha -> beta
S = start symbol from N

Penn Treebank dataset

Leftmost derivation.

Classic Parsing Methods:
Parsing As Search
There are two types of constraints on the parses:
a. From the input sentence
b. From the grammar

Two general approaches to parsing:
a. Top-down
b. Bottom-up

Bottom-up:
explores options that won't lead to a full parse.

Top-up:
explores options that don't match the full sentence.

Dynamic Programming:
caches of intermediate results

Cocke-Kasami-Younger (CKY) Parser
based on dynamic programming, and Chomsky Normal Grammar

Shift-reduce parsing:
A bottom-up parser: tries to match the RHS of a production until it can bind an S
Shift operation: Each word in the input sentence is pushed onto a stack.
Reduce operation: If the top n words on the top of the stack match the RHS of a production, then they are popped and replaced by the LHS of the production
Stopping operation: The process stops when the input sentence has been processed and S has been popped from the stack.

Dynamic Programming:
Motivation:
a. A lot of work is repeated.
b. Caching intermediate results improves the complexity.

DP:
Building a parse for a substring [i, j] based on all parses [i, k] and [k, j] that are included in it.

Complexity:
O(n^3)

Issues with CKY:
Weak equivalence only
a. Same language, different structure
b. If the grammar had to be converted to CNF, then the final parse tree doesn't match the original grammar.
c. However, it can be converted back using a specific procedure.

Syntactic ambiquity:
a. CKY has no way to perform syntactic disambiguation.

Earley Parser:
No need to convert the grammar to CNF
Left to right

Complexity:
Faster than O(n^3)

Looks for both full and partial consituents.
For e.g S -> Aux. NP VP

When reading word k, it has already identified all hypothesis that are consistent with words 1 to k-1

It uses a dynamic programming table, just like CKY

Problems in dynamic programming parser:
a. Agreement
Many combinations of rules are needed to express agreement.
b. Non-independence
Use lexicalised grammar

Syntax helps understand the meaning of a sentence.

The Penn Treebank:
Used for supervised learning methods.
Size: 40k training sentences
2400 test sentences

Penn Treebank tagset

Classification task:
a. Document Retrival
b. Part of Speech tagging
c. Parsing

Data Split:
Training
Validation
Test

Evaluation methods:
a. Accuracy
b. Precision and Recall
c. F1: harmonic mean of precision and recall

================================================================================================

Week 5
======
Syntax and Parsing Part 2

Parsing noun sequences:
Parsing programming languages are unambigous.

Parsing human languages is ambigous:
a. Coordination scope: Small boys and girls are playing.
b. Prepositional phrase attachment: I saw the man with the telescope.
c. Gaps: Mary likes Physics but hates Chemistry.
d. Particles vs. Prepositions: She ran up a large bill.
e. Gerund vs Adjectives: Playing cards can be expensive.

Noun-noun compounds:
Fish tank = tank that holds fish
Fish net = net used to catch fish
Fish soup = soup made with fish

Head of the compound:
Second word usually. 
For e.g College junior - a kind of junior

Prepositional Phrase Attachment 1/3:
a. High (verbal): join board as director
b. Low (nominal): is chairman of Elsevier

Binary classification problem:
Input: a prepositional phrase and the surrounding context
Output: a binary label: 0 (high) or 1(low)
In practice: the context consists only of four words: the preposition, the verb before the preposition, the noun before the preposition, and the noun after the preposition

Dataset: RRR94 (Part of Penn treebank)

Supervised learning: Evaluation
a. Manually label a set of instances.
b. Split the labeled data into training and testing sets.
c. Use the training data to find patterns.
d. Apply these patterns on the testing dataset.
e. For evaluation: use accuracy

Prepositional Phrase Attachment 2/3:
Find the lower bound and upper bound on accuracy.
Lower bound: Classifying everything with one class(Popular class in the test dataset).
Upper bound: Human performance

Using Linguistic knowledge to create more features.
Using various decision rules to create a classifier.

Prepositional Phrase Attachment 3/3:
Using maximum likelihood estimate for classifying prepositional phrase attachment.

Statistical Parsing:
Need for Probabilistic Parsing:
Need for ranking the parse trees

Probabilistic Context-Free Grammars (PCFG)

The probability of a parse tree t given n productions used to build it
p(t) = Pie P(alpha_i -> beta_i)

The most likely parse is
arg max p(t)

Probabilistic Parsing Methods:
a. Probabilistic Earley algorithm
b. Probabilistic Cocke-Kasami-Younger (CKY) algorithm

Probabilistic Grammars:
Learned from training corpus

Maximum Likelihood Estimates:
P(alpha -> beta) = Count(alpha -> beta) / Count(alpha)

Lexicalised Parsing:
Limitation of PCFG:
a. The probabilities don't depend on the specific words.
b. It is not possible to disambiguate sentences based on semantic information.

Using head of a phrase as an additional source of information.

Collin's Parser for Lexicalised Parsing

Issues with Lexicalised Parsing:
a. Sparseness of training data
b. Combinatorial explosion

Discriminative Reranking:
A parser may return many parses of sentence, with small differences in probabilities.
Other considerations may need to be taken into account:
a. Parse tree depth
b. left attachment or right attachment
c. discourse structure

Dependency Parsing:

Dependency structure:
For e.g blue house
blue: modifier, dependent, child, subordinate
house: head, governor, parent, regent

Sentence Parsing (Starts with S)
Dependency Parsing (Starts with verb)

Techniques:
a. Dynamic Programming O(n^5)
CKY Parser

b. Constraint based methods O(n^3)
NP Complete problem

c. Deterministic Parsing O(n^2)
Graph-based methods
Maximum Spanning trees

English dependency trees are mostly projective (can be drawn without crossing dependencies).
Other languages are not.

Dependency parsing is equivalent to search for maximum spanning tree in a directed graph.

e. Malt Parser O(n)
Very similar to shift-reduce parsing

Application:
a. Information Extraction
b. Dependency Kernels

Alternative Parsing Formalism:

Mildly Context-Sensitive Grammars:
a. Tree Substitution Grammar (TSG)
Terminals generate entire tree fragments.
TSG and CFG are formally equivalent.

b. Tree Adjoining Grammar (TAG)
Like TSG, but allow adjunction
It can generate languages like a^n b^n c^n
For e.g
Mary gave a book and a magazine to Chen and Mike, respectively.
TAG is formally more powerful than CFG
TAG is less powerful than CSG

c. Combinatory Categorial Grammar (CCG)
Complex types
For e.g X\Y and X/Y
These takes an argument of type Y and return an object of type X.
X\Y - means that Y should appear on the left.
X/Y - means that Y should appear on the right.

For e.g
I sleep

CCG can generate the language a^n b^n c^n d^n

Semantic Parsing:
Associate a semantic expression with each node.

For e.g
Javier eats pizza.

Parse tree
N: Javier
V: lambda, x, y, eat(x, y)
N: pizza
VP: lambda, x, y, eat(x, pizza)
S: eat(Javier, pizza)

Application:
Question Answering
Bots

==================================================================================================

Week 6
======
Language Modeling

Probabilities:
Probabilistic reasoning is very important in NLP.
Applications:
Speech recognition
Machine Translation

Probabilities make it possible to combine evidence from multiple sources in a systematic way.

Termilogies:
a. Probability theory
b. Experiment
c. Possible outcomes
d. Same spaces
e. Events
f. Probability distribution

Meaning of probabilites:
a. Frequentist
I threw the coin 10 times and it turned up heads 5 times.
b. Subjective
I am willing to bet 50 cents on heads.

Conditional Probability:
Prior and posterior probability
P(A|B) = P(A intersection B) / P(B)

The chain rule:
P(w1, w2, ... wn) = P(w1) P(w2|w1) P(w3|w2, w1) ... P(wn|w1 ... wn-1)
or Pie P(wi|w1 ... wi-1)

Used in Markov models.

Independence:
P(A intersection B) = P(A) P(B)
Unless P(B) is 0,
P(A) = P(A|B)

Adding and Removing constraints (Backoff) while calculating conditional probabilities with multiple joint probabilities.
Adding: More accurate, but need lot of data.

Random Variables:
The numbers are generated by a stochastic process with a certain probability distribution.

Probability mass function:
Probability that the random variable has different numeric values.
P(x) = P(X = x)

Bayes Theorem:
P(A, B) = P(B|A) P(A)
P(A, B) = P(A|B) P(B)

P(B|A) = P(A|B) P(B) / P(A)

Language Modeling:

Probabilistic Language Models:
Assign a probability to a sentence
P(S) = P(w1, w2, ..., wn)

Application:
Predicting the next word in the sequence
P(wn|w1 ... wn-1)

Speech recognition
Text generation
Spelling correction
Machine translation
OCR
Summarization
Document classification

P(w1, w2, ..., wn) = P(w1) P(w2|w1) P(w3|w1, w2) ... P(wn|w1, ... wn-1)

N-gram model:
Markov assumption - Only look at limited history
Unigram
Bigram
Trigram

Brown corpus

Maximum Likelihood Estimates:
Use training data
Count how many times a given context appears in it.

These estimates may not be good for corpora from other genres.

Special symbols to start the sentence and end.

N-grams and Regular Languages:
N-grams are just one way to represent weighted regular languages.

Generative Model

Use logarithms (base 10) to avoid underflow of small probabilities.
Use sum instead of product.

Smoothing:
Too many parameters to estimate even a unigram model.
MLE assigns values of 0 to unseen data.

Main idea:
Reassigning some probability mass to unseen data.
Distributing some of the probability mass to allow for novel events.

Add-one (Laplace) smoothing:
Bigrams: P(wi|wi-1) = ((C(wi-1, wi) + 1) / C(wi-1) + V)

Good turing:
Revised counts C* = (c+1) Nc+1/Nc

Kneser-ney
Class-based n-grams

Backoff:
Going back to the lower-order n-gram model if the higher-order model is sparse.

Interpolation:
P'(wi|wi-2, wi-1) = lambda1 * P(wi|wi-2, wi-1) + lambda2 * P(wi|wi-1) + lambda3 * P(wi)
