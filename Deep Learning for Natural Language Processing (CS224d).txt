Deep Learning for Natural Language Processing (CS224d)
======================================================

Lecture 1
=========
Intro to NLP and Deep Learning

NLP:
Field intersection of
a. Computer Science
b. AI
c. Linguistics

Goal: Computers to process or "understand" natural language in order to perform tasks.
For e.g Question Answering

Perfect language understanding is AI-Complete.

NLP Levels:
Speech -> Phonological Analysis / Text -> Tokenization
Morphological Analysis
Syntactic Analysis
Semantic Analysis
Discourse Processing

Applications:
Spell checking, keyword search
Information Extraction
Machine Translation, Question Answering

NLP Hard:
Complexity in representating, learning and using linguistic/situational/world/visual knowledge.
Ambiguity

Deep Learning:
Model optimizing weights to best make a final prediction

Machine learning vs Deep learning:
Machine Learning: 90-95 percent of time is spend in describing the data with features that computer can understand.
Deep Learning: Representation attempts to automatically learn good features or representations.

Deep Learning can learn supervised as well as unsupervised.

Why Deep Learning now?
DL techniques benefit more from a lot of data.
Faster machines and multicore CPU/GPU help DL.
New models, algorithms, ideas.

Deep Learning + NLP = Deep NP
Combine ideas and goals of NLP and use representation learning and deep learning methods to solve them.

Deep NLP levels:
a. Phonology:
Tradional: Phonemes
DL: trains to predict phonemes (or words directly) from sound features and represent them as vectors.

b. Morphology:
Traditional: Morphemes; Prefix, suffix, stem
DL: Every morpheme is a vector. A neural network combines two vectors into one vector.

c. Syntax:
Traditional: Phrases, Discrete categories like NP, VP
DL: Every word and every phrase is a vector
A neural network combines two vectors into one vector

d. Semantics:
Traditional: Lambda calculus; First order logic; carefully engineered functions, takes an inputs specific other functions
DL: Every word and every phrase and every logical expression is a vector.
A neural network combines two vectors into one vector.

Applications:
a. Sentiment Analysis:
Traditional: Curated sentiment dictionaries combined with either bag-of-words representations or hand-designed negation features.
DL: Same deep learning model that was used for morphology, syntax, and logical semantics can be used. RNN!

b. Question Answering:
Traditional: A lot of feature engineering to capture world and other knowledge. Regex
DL: Same model can be used. RNN!

c. Machine Translation:
Traditional: Interlingua, IBM Model 1 & 2, Phrase based models
DL: RNN! Sequence to sequence model.

Representation for all levels: Vectors

==========================================================================================================================

Lecture 2
=========
Simple Word Vector representations: word2vec, GloVe

Represent meaning in a computer:
Use a taxonomy like WordNet that has hypernyms (is-a) relationships.

Problems with this discrete representation:
Missing nuances
Missing new words
Subjective
Requires human labor to create and adapt
Hard to compute accuracy word similarity

Distributional similarity based representations:
Representing a word by means of its neighbors.

How to make neighbors represent words?
With a cooccurrence matrix X
a. Full document: Matrix X will give general topics leading to "Latent Semantic Analysis".
b. Window: Matrix captures syntactic (POS) and semantic information.

Window size: 1
Each word has its own matrix representation.
Matrix consists of count of neighboring words in the window size.

Problems with simple cooccurrence vectors:
Increase in size with vocabulary
Very high dimensional
Classification models have sparsity issues.
Models are less robust.

Solution: Low dimensional vectors

Method 1: Dimensionality Reduction on X
Singular Value Decomposition

Problems with SVD:
Computational cost
Bad for millions of words or documents
Hard to incorporate new words or documents

Idea: Directly learn low-dimensional word vectors
Old ideas:
Learning representations by back-propagating errors.
A neural probabilistic language model
word2vec

Main idea of word2vec:
Instead of capturing cooccurence counts directly, predict surrouding words of every word.
Faster and can easily incorporate a new sentence/document or add a word to the vocabulary.

Details of word2vec:
Predict surrounding words in a window of length m of every word.
Object function: Maximize the log probability of any context word given the current center word:
J(theta) = 1/T Sigma Sigma log p(wt+j|wt) where t = 1 to T and -m <= j <= m

For p(wt+j|wj), the simplest form is
p(o|c) = e^(u * v) / Sigma e^(u * v)

where
o: outside word id
c: center word id
u: center and outside word vector of o
v: center and outside word vector of c

Every word has two vector.
This is essentially "dynamic" logistic regression.

Objective/Cost functions:
Gradient descent

Approximations:
With large vocabularies, objective function is not scalable and would train too slowly.
Idea:
Approximate the normalization
Define negative prediction that only samples a few words that do not appear in the context.
Similar to focusing on mostly positive correlations.

Linear Relationships in word2vec:
Syntactically:
X(apple) - X(apples) = X(car) - X(cars) = X(family) - X(families)

Semantically:
X(shirt) - X(clothing) = X(chair) - X(furniture)

Count based vs direct prediction:
Count based: LSA, HAL, PCA
Fast training
Efficient usage of statistics
Primarily used to capture word similarity
Disproportionate importance given to large counts

Direct Prediction: RNN, Skip/gram, CBOW
Scales with corpus size
Inefficient usage of statistics
Generate improved performance on other tasks
Can capture complex patterns beyond word similarity

=================================================================================================================

Lecture 3
=========
Advanced word vector representations: language models, softmax, single layer networks

Word vectors:
Define the set of all parameters in a model in terms of one long vector theta

Gradient Descent:
To minimize J(theta) over the full batch would require us to compute gradients for all windows.

Updates for each element of theta:
theta_new = theta_old - alpha d J(theta)/d(theta_old)

Stochastic Gradient Descent:
We will update parameters after each window t.

Approximations: The skip-gram model
Main idea: train binary logistic regressions for a true pair (center word and word in its context window) and a couple of random pairs.

Continous bag of words model:
Main idea: Predict center word from sum of surrounding word vectors.

GloVe:
J(theta) = 1/2 Sigma f(Pij) (ui * vj - log Pij)^2

Good performance with small corpora.

What to do with the two sets of vectors?
U and V captures similar co-occurrence information. 
Xfinal = U + V

Evaluate word vectors:

a. Intrinsic:
Evaluation on a specific/intermediate subtasks
Fast to compute
Helps to understand the system

For e.g
Word vector analogies
a:b :: c:?
Human judgements

b. Extrinsic:
Evaluation on a real task
Can take a long time to compute accuracy.

For e.g
Name-entity recognition

Hyperparameters:
Best dimensions: 300
Window size: 8 is good for GloVe
Wikipedia corpus is better for training, but common crawl is better.

Handling ambiguity:
Idea: Cluster word windows around words, restrain with each word assigned to multiple different clusters bank1, bank2, etc.

Logistic regression: Softmax classification on word vector x to obtain probabilities for class y.
Loss function: Cross entropy

=======================================================================================================================

Lecture 4
=========
Word window classification and Neural Networks

Classification setup and notation:
Generally, we have a training dataset consisting of samples
{xi yi}^N

where xi: inputs, e.g words, context windows, sentences, documents, etc
yi: labels, sentiment, named entities, buy/sell decision, multi-word sequences

Softmax layer with cross entropy loss function with regularization for classification problems.

Learn both W and word vectors x
But, re-training word vectors leads to loss of generalisation because words in training data will move around.

Main idea:
If you only have a small training dataset, don't train the word vectors.

Word vectors = Word embeddings = Word representations

Window Classification:
Idea: classify a word in its context window of neighboring words.
For e.g named entity recognition into 4 classes

Train softmax classifier by assigning a label to a center word and concatenating all word vectors surrounding it.

Softmax (logistic regression) is not very powerful, because they are linear.

Neural networks can learn much more complex functions and non-linear decision boundaries.

Neuron:
h(x) = f(wx + b)
f(z) = 1/1+e^z

Basic idea of neural network:
It is the loss function that will direct what the intermediate hidden variables should be, so as to do a good job at predicting the target for the next layer.

============================================================================================================================

Lecture 5
=========
Neural Networks & Backprop

Project types:
a. Apply existing neural network model to a new task. For e.g Summarization
b. Implement a complex neural architecture
c. Come up with a new neural network model
d. Theory of deep learning, e.g optimization

Project steps:
1. Define task
2. Define dataset
3. Define your metric
4. Split your dataset
5. Establish a baseline
6. Implement existing neural net model
7. Always be close to your data
8. Try out different model variants

Max-margin loss function:
J = max(0, 1 - s + sc)
where s is score of positive window
sc is score of negative window

Training with Backpropagation.

=======================================================================================================================

Week 6
======
Recurrent Neural Networks

Backpropagation:
Compute gradient of example-wise loss with respect to parameters

Computing the loss is O(n)

Neural tips/tricks:
Multitask models: Share bottom layers of a neural network for other problems.

Use unsupervised word vector per-training on a large text collection.

General Strategy:
1. Select network structure appropriate for problem
Single word, fixed windows, sentence based, document level, bag of words, recursive vs recurrent, CNN
Non-linearity: Sigmoid, tanh, ReLu

2. Check for implementation bugs with gradient checks
Start from simplest model then go to what you want.

3. Parameter initialisation
Initialize hidden layer biases to 0 and output biases to optimal value if weight were 0
Initialize weights Uniform(-r, r), r inversely propositional to fan-in and fan-out
(6/(fan-in + fan-out))^1/2

4. Optimization tricks
Use Mini-Batch Stochastic Gradient Descent
Momentum with reduction in global learning rate.
Variable learning rate
AdaGrad: Adaptive learning rate for each parameter

5. Avoid overfitting or underfitting.
Reduce model size
Standard l1 and l2 regularization
Early stopping
Dropout

Language Models:
Computes a probability for a sequence of words.
P(w1, w2, ... wn) = Pie (wi|w1, w2, .... wi-1)

Trigram/Bigra/Unigram models

Fixed window model of context can be used for language modeling.

Recurrent Neural Networks:
Condition the neural networks on all previous words and tie the weights at each time step.
ht = f(W(hh)ht-1 + W(hx)xt)
We use the same set of W weights at all time steps.

Cross entropy error (l) for training.

Evaluation: Perplexity: 2^l

Training RNN is hard:
Vanishing or exploding gradient problem.

======================================================================================================================

Week 7
======
Tensorflow

Deep Learning Package Design Choices:

Model specification:
a. Configuration file: Caffe, DistBelief, CNTK
b. Programmatic generation: Torch, Theano, Tensorflow

Programmatic models:
Lua: Torch
Python: Theano, Tensorflow
Others

Theano is an academic project, whereas Tensorflow funded by Google.

Tensor:
Multilinear maps from vector spaces to the real numbers.

Tensorflow vs Numpy:
Numpy has Ndarray support, but doesn't offer methods to create tensor functions and automatically compute derivates.
No GPU support too.

Tensorflow requires explicit evaluation.
ta.eval()

Workflow:
Create a graph
Create a environment (Session) tf.Session()
Link the graph to the environment
Execute the graph

All computations add notes to global default graph.

tf.InteractiveSession() is just convenient syntactic sugar for keeping a default session open.
sess.run(c) is example of Tensorflow Fetch.

Tensorflow Variables
tf.Variable()
tf.initialize_all_variables()

Convert numpy array to tensor:
tf.convert_to_tensor(a)

Placeholders and feed dictionaries:
Use tf.placeholder variables (dummy nodes that provide entry points for data to computational graph)
A feed_dict is a python dictionary mapping from tf.placeholder vars to data.

Variable scope:
tf.variable_scope()
tf.get_variable()

Reuse variables scope:
tf.get_variable_scope().reuse_variables()

Tensorboard:
Build-in visualisations

==================================================================================================================
