Deep Learning for Natural Language Processing (CS224d)
======================================================

Lecture 1
=========
Intro to NLP and Deep Learning

NLP:
Field intersection of
a. Computer Science
b. AI
c. Linguistics

Goal: Computers to process or "understand" natural language in order to perform tasks.
For e.g Question Answering

Perfect language understanding is AI-Complete.

NLP Levels:
Speech -> Phonological Analysis / Text -> Tokenization
Morphological Analysis
Syntactic Analysis
Semantic Analysis
Discourse Processing

Applications:
Spell checking, keyword search
Information Extraction
Machine Translation, Question Answering

NLP Hard:
Complexity in representating, learning and using linguistic/situational/world/visual knowledge.
Ambiguity

Deep Learning:
Model optimizing weights to best make a final prediction

Machine learning vs Deep learning:
Machine Learning: 90-95 percent of time is spend in describing the data with features that computer can understand.
Deep Learning: Representation attempts to automatically learn good features or representations.

Deep Learning can learn supervised as well as unsupervised.

Why Deep Learning now?
DL techniques benefit more from a lot of data.
Faster machines and multicore CPU/GPU help DL.
New models, algorithms, ideas.

Deep Learning + NLP = Deep NP
Combine ideas and goals of NLP and use representation learning and deep learning methods to solve them.

Deep NLP levels:
a. Phonology:
Tradional: Phonemes
DL: trains to predict phonemes (or words directly) from sound features and represent them as vectors.

b. Morphology:
Traditional: Morphemes; Prefix, suffix, stem
DL: Every morpheme is a vector. A neural network combines two vectors into one vector.

c. Syntax:
Traditional: Phrases, Discrete categories like NP, VP
DL: Every word and every phrase is a vector
A neural network combines two vectors into one vector

d. Semantics:
Traditional: Lambda calculus; First order logic; carefully engineered functions, takes an inputs specific other functions
DL: Every word and every phrase and every logical expression is a vector.
A neural network combines two vectors into one vector.

Applications:
a. Sentiment Analysis:
Traditional: Curated sentiment dictionaries combined with either bag-of-words representations or hand-designed negation features.
DL: Same deep learning model that was used for morphology, syntax, and logical semantics can be used. RNN!

b. Question Answering:
Traditional: A lot of feature engineering to capture world and other knowledge. Regex
DL: Same model can be used. RNN!

c. Machine Translation:
Traditional: Interlingua, IBM Model 1 & 2, Phrase based models
DL: RNN! Sequence to sequence model.

Representation for all levels: Vectors

==========================================================================================================================

Lecture 2
=========
Simple Word Vector representations: word2vec, GloVe

Represent meaning in a computer:
Use a taxonomy like WordNet that has hypernyms (is-a) relationships.

Problems with this discrete representation:
Missing nuances
Missing new words
Subjective
Requires human labor to create and adapt
Hard to compute accuracy word similarity

Distributional similarity based representations:
Representing a word by means of its neighbors.

How to make neighbors represent words?
With a cooccurrence matrix X
a. Full document: Matrix X will give general topics leading to "Latent Semantic Analysis".
b. Window: Matrix captures syntactic (POS) and semantic information.

Window size: 1
Each word has its own matrix representation.
Matrix consists of count of neighboring words in the window size.

Problems with simple cooccurrence vectors:
Increase in size with vocabulary
Very high dimensional
Classification models have sparsity issues.
Models are less robust.

Solution: Low dimensional vectors

Method 1: Dimensionality Reduction on X
Singular Value Decomposition

Problems with SVD:
Computational cost
Bad for millions of words or documents
Hard to incorporate new words or documents

Idea: Directly learn low-dimensional word vectors
Old ideas:
Learning representations by back-propagating errors.
A neural probabilistic language model
word2vec

Main idea of word2vec:
Instead of capturing cooccurence counts directly, predict surrouding words of every word.
Faster and can easily incorporate a new sentence/document or add a word to the vocabulary.

Details of word2vec:
Predict surrounding words in a window of length m of every word.
Object function: Maximize the log probability of any context word given the current center word:
J(theta) = 1/T Sigma Sigma log p(wt+j|wt) where t = 1 to T and -m <= j <= m

For p(wt+j|wj), the simplest form is
p(o|c) = e^(u * v) / Sigma e^(u * v)

where
o: outside word id
c: center word id
u: center and outside word vector of o
v: center and outside word vector of c

Every word has two vector.
This is essentially "dynamic" logistic regression.

Objective/Cost functions:
Gradient descent

Approximations:
With large vocabularies, objective function is not scalable and would train too slowly.
Idea:
Approximate the normalization
Define negative prediction that only samples a few words that do not appear in the context.
Similar to focusing on mostly positive correlations.

Linear Relationships in word2vec:
Syntactically:
X(apple) - X(apples) = X(car) - X(cars) = X(family) - X(families)

Semantically:
X(shirt) - X(clothing) = X(chair) - X(furniture)

Count based vs direct prediction:
Count based: LSA, HAL, PCA
Fast training
Efficient usage of statistics
Primarily used to capture word similarity
Disproportionate importance given to large counts

Direct Prediction: RNN, Skip/gram, CBOW
Scales with corpus size
Inefficient usage of statistics
Generate improved performance on other tasks
Can capture complex patterns beyond word similarity

=================================================================================================================

Lecture 3
=========
Advanced word vector representations: language models, softmax, single layer networks

Word vectors:
Define the set of all parameters in a model in terms of one long vector theta

Gradient Descent:
To minimize J(theta) over the full batch would require us to compute gradients for all windows.

Updates for each element of theta:
theta_new = theta_old - alpha d J(theta)/d(theta_old)

Stochastic Gradient Descent:
We will update parameters after each window t.

Approximations: The skip-gram model
Main idea: train binary logistic regressions for a true pair (center word and word in its context window) and a couple of random pairs.

Continous bag of words model:
Main idea: Predict center word from sum of surrounding word vectors.

GloVe:
J(theta) = 1/2 Sigma f(Pij) (ui * vj - log Pij)^2

Good performance with small corpora.

What to do with the two sets of vectors?
U and V captures similar co-occurrence information. 
Xfinal = U + V

Evaluate word vectors:

a. Intrinsic:
Evaluation on a specific/intermediate subtasks
Fast to compute
Helps to understand the system

For e.g
Word vector analogies
a:b :: c:?
Human judgements

b. Extrinsic:
Evaluation on a real task
Can take a long time to compute accuracy.

For e.g
Name-entity recognition

Hyperparameters:
Best dimensions: 300
Window size: 8 is good for GloVe
Wikipedia corpus is better for training, but common crawl is better.

Handling ambiguity:
Idea: Cluster word windows around words, restrain with each word assigned to multiple different clusters bank1, bank2, etc.

Logistic regression: Softmax classification on word vector x to obtain probabilities for class y.
Loss function: Cross entropy

=======================================================================================================================

Lecture 4
=========
Word window classification and Neural Networks

Classification setup and notation:
Generally, we have a training dataset consisting of samples
{xi yi}^N

where xi: inputs, e.g words, context windows, sentences, documents, etc
yi: labels, sentiment, named entities, buy/sell decision, multi-word sequences

Softmax layer with cross entropy loss function with regularization for classification problems.

Learn both W and word vectors x
But, re-training word vectors leads to loss of generalisation because words in training data will move around.

Main idea:
If you only have a small training dataset, don't train the word vectors.

Word vectors = Word embeddings = Word representations

Window Classification:
Idea: classify a word in its context window of neighboring words.
For e.g named entity recognition into 4 classes

Train softmax classifier by assigning a label to a center word and concatenating all word vectors surrounding it.

Softmax (logistic regression) is not very powerful, because they are linear.

Neural networks can learn much more complex functions and non-linear decision boundaries.

Neuron:
h(x) = f(wx + b)
f(z) = 1/1+e^z

Basic idea of neural network:
It is the loss function that will direct what the intermediate hidden variables should be, so as to do a good job at predicting the target for the next layer.

============================================================================================================================

Lecture 5
=========
Neural Networks & Backprop

Project types:
a. Apply existing neural network model to a new task. For e.g Summarization
b. Implement a complex neural architecture
c. Come up with a new neural network model
d. Theory of deep learning, e.g optimization

Project steps:
1. Define task
2. Define dataset
3. Define your metric
4. Split your dataset
5. Establish a baseline
6. Implement existing neural net model
7. Always be close to your data
8. Try out different model variants

Max-margin loss function:
J = max(0, 1 - s + sc)
where s is score of positive window
sc is score of negative window

Training with Backpropagation.

=======================================================================================================================

Lecture 6
=========
Recurrent Neural Networks

Backpropagation:
Compute gradient of example-wise loss with respect to parameters

Computing the loss is O(n)

Neural tips/tricks:
Multitask models: Share bottom layers of a neural network for other problems.

Use unsupervised word vector per-training on a large text collection.

General Strategy:
1. Select network structure appropriate for problem
Single word, fixed windows, sentence based, document level, bag of words, recursive vs recurrent, CNN
Non-linearity: Sigmoid, tanh, ReLu

2. Check for implementation bugs with gradient checks
Start from simplest model then go to what you want.

3. Parameter initialisation
Initialize hidden layer biases to 0 and output biases to optimal value if weight were 0
Initialize weights Uniform(-r, r), r inversely propositional to fan-in and fan-out
(6/(fan-in + fan-out))^1/2

4. Optimization tricks
Use Mini-Batch Stochastic Gradient Descent
Momentum with reduction in global learning rate.
Variable learning rate
AdaGrad: Adaptive learning rate for each parameter

5. Avoid overfitting or underfitting.
Reduce model size
Standard l1 and l2 regularization
Early stopping
Dropout

Language Models:
Computes a probability for a sequence of words.
P(w1, w2, ... wn) = Pie (wi|w1, w2, .... wi-1)

Trigram/Bigra/Unigram models

Fixed window model of context can be used for language modeling.

Recurrent Neural Networks:
Condition the neural networks on all previous words and tie the weights at each time step.
ht = f(W(hh)ht-1 + W(hx)xt)
We use the same set of W weights at all time steps.

Cross entropy error (l) for training.

Evaluation: Perplexity: 2^l

Training RNN is hard:
Vanishing or exploding gradient problem.

======================================================================================================================

Lecture 7
=========
Tensorflow

Deep Learning Package Design Choices:

Model specification:
a. Configuration file: Caffe, DistBelief, CNTK
b. Programmatic generation: Torch, Theano, Tensorflow

Programmatic models:
Lua: Torch
Python: Theano, Tensorflow
Others

Theano is an academic project, whereas Tensorflow funded by Google.

Tensor:
Multilinear maps from vector spaces to the real numbers.

Tensorflow vs Numpy:
Numpy has Ndarray support, but doesn't offer methods to create tensor functions and automatically compute derivates.
No GPU support too.

Tensorflow requires explicit evaluation.
ta.eval()

Workflow:
Create a graph
Create a environment (Session) tf.Session()
Link the graph to the environment
Execute the graph

All computations add notes to global default graph.

tf.InteractiveSession() is just convenient syntactic sugar for keeping a default session open.
sess.run(c) is example of Tensorflow Fetch.

Tensorflow Variables
tf.Variable()
tf.initialize_all_variables()

Convert numpy array to tensor:
tf.convert_to_tensor(a)

Placeholders and feed dictionaries:
Use tf.placeholder variables (dummy nodes that provide entry points for data to computational graph)
A feed_dict is a python dictionary mapping from tf.placeholder vars to data.

Variable scope:
tf.variable_scope()
tf.get_variable()

Reuse variables scope:
tf.get_variable_scope().reuse_variables()

Tensorboard:
Build-in visualisations

==================================================================================================================

Lecture 8
=========
Recurrent Neural Networks

Language Models:
A language model computes a probability for a sequence of words. P(w1, w2, ... wn)
Useful for Machine Translation

Traditional language models:
Using Markov assumption, create unigram, bigram, or trigram models

Performance improves with keeping around higher n-grams counts and doing smoothing and so called backoff.

Recurrent Neural Networks:
RNNs tie the weights at each time step.
Condition the neural network on all previous words.

Model:
Given list of word vectors:
x1 ... xt-1, xt, xt+1, ... xn

At a single time step
ht = f(W(hh)ht-1 + W(hx)xt)
yt = softmax(W(s)ht)

For language model
P(xt+1 = vj|x1, ... xt) = ytj

Cross entropy loss function but predicting words instead of class.

Evaluation:
Perplexity (Lower is better)

Training RNNs is hard:
Multiply the same matrix at each time step during forward prop.
Ideally inputs from many time step ago can modify output y.

Vanishing/exploding gradient problem:
It is due to matrix matrix derivates and multiplications in hidden-hidden units.

Why is the vanishing gradient a problem?
The error at a time step ideally can tell a previous time step from many steps away to change during backprop.

Trick for exploding gradient: clipping trick
Clip gradients to a maximum value

For vanishing gradients: Initialization + ReLus
Initialize W(*) to identity matrix

Note:
You only need to pass backwards through your sequence once and accumulate all the deltas from each Et.
Class-based word prediction.
P(wt|history) = P(ct|history) P(wt|ct)

Other task:
Classify each word into:
a. NER
b. Entity level sentiment in context
c. Opinionated expressions

Bidirectional RNN:
For classification you want to incorporate information from words both preceding and following.
P(wi|w1 ... wi-1, wi+1, ... wn)

Deep bidirectional RNN for more powerful problems.
Three layers are sufficient for better f1 score.

Evaluation:
Precision, Recall, and F1

=========================================================================================================================

Lecture 9
=========
Fancy Recurrent Neural Networks for Machine Translation

Deep Bidirectional RNN:
Dataset: MPQA 1.2 corpus consists of 535 news articles.

Machine Translation:
Methods are statistical.
Use parallel corpora.

Probabilistic model:
e = arg max P(e|f) = arg max P(f|e) P(e)

Step1: Alignment
It is itself a challenging problem.

Step2: Search

Deep learning model:
Sequence to sequence RNN

RNN Translation Model Extensions:
a. Train different RNN weights for encoding and decoding.
b. Compute every hidden state in decoder from
	i. Previous hidden state
	ii. Last hidden vector of encoder.
	iii. Previous predicted output word
c. Train stacked/Deep RNN with multiple layers.
d. Potentially train bidirectional encoder.
e. Train input sequence in reverse order for simple optimization problem.
f. Better units

GRU (Gated Recurrent Units):
Main ideas:
Keep around memories to capture long distance dependencies.
Allow error messages to flow at different strengths depending on the inputs.

Model:
GRU first computes an update gate (another layer) based on current input word vector and hidden state.
zt = sigmoid(W(z)xt + U(z)ht-1)

Compute reset gate similarly but with different weights.
rt = sigmoid(W(r)xt + U(r)ht-1)

New memory content
ht = tanh(Wxt + rt U ht-1)
If reset gate unit is 0, then this ignores previous memory and only stores the new word information.

Final memory at time step combines current and previous time steps:
ht = zt ht-1 + (1 - zt) ht

Intuition:
If reset is close to 0, ignore previous hidden state.
- Allows model to drop information that is irrelevant in the future.
Update gate controls how much of past state should matter now
- If z is close to 1, then we can copy information in that unit through many time steps. Less vanishing gradient.

Long-short-term-memories (LSTMs):
Allow each time step to modify:
a. Input gate: i = sigmoid(Wi x + Ui ht-1)
b. Forget gate: f = sigmoid(Wf x + Uf ht-1)
c. Output gate: o = sigmoid(Wo x + Uo ht-1)
d. New memory cell: ct = tanh(Wc x + Uc ht-1)

Final memory cell: ct = ft ct-1 + i(t) ct
Final hidden state: ht = o(t) + tanh(ct)

Deep LSTMs don't outperform traditional MT yet.

=========================================================================================================================
