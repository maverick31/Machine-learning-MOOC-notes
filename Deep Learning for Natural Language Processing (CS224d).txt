Deep Learning for Natural Language Processing (CS224d)
======================================================

Lecture 1
=========
Intro to NLP and Deep Learning

NLP:
Field intersection of
a. Computer Science
b. AI
c. Linguistics

Goal: Computers to process or "understand" natural language in order to perform tasks.
For e.g Question Answering

Perfect language understanding is AI-Complete.

NLP Levels:
Speech -> Phonological Analysis / Text -> Tokenization
Morphological Analysis
Syntactic Analysis
Semantic Analysis
Discourse Processing

Applications:
Spell checking, keyword search
Information Extraction
Machine Translation, Question Answering

NLP Hard:
Complexity in representating, learning and using linguistic/situational/world/visual knowledge.
Ambiguity

Deep Learning:
Model optimizing weights to best make a final prediction

Machine learning vs Deep learning:
Machine Learning: 90-95 percent of time is spend in describing the data with features that computer can understand.
Deep Learning: Representation attempts to automatically learn good features or representations.

Deep Learning can learn supervised as well as unsupervised.

Why Deep Learning now?
DL techniques benefit more from a lot of data.
Faster machines and multicore CPU/GPU help DL.
New models, algorithms, ideas.

Deep Learning + NLP = Deep NP
Combine ideas and goals of NLP and use representation learning and deep learning methods to solve them.

Deep NLP levels:
a. Phonology:
Tradional: Phonemes
DL: trains to predict phonemes (or words directly) from sound features and represent them as vectors.

b. Morphology:
Traditional: Morphemes; Prefix, suffix, stem
DL: Every morpheme is a vector. A neural network combines two vectors into one vector.

c. Syntax:
Traditional: Phrases, Discrete categories like NP, VP
DL: Every word and every phrase is a vector
A neural network combines two vectors into one vector

d. Semantics:
Traditional: Lambda calculus; First order logic; carefully engineered functions, takes an inputs specific other functions
DL: Every word and every phrase and every logical expression is a vector.
A neural network combines two vectors into one vector.

Applications:
a. Sentiment Analysis:
Traditional: Curated sentiment dictionaries combined with either bag-of-words representations or hand-designed negation features.
DL: Same deep learning model that was used for morphology, syntax, and logical semantics can be used. RNN!

b. Question Answering:
Traditional: A lot of feature engineering to capture world and other knowledge. Regex
DL: Same model can be used. RNN!

c. Machine Translation:
Traditional: Interlingua, IBM Model 1 & 2, Phrase based models
DL: RNN! Sequence to sequence model.

Representation for all levels: Vectors

==========================================================================================================================

Lecture 2
=========
Simple Word Vector representations: word2vec, GloVe

Represent meaning in a computer:
Use a taxonomy like WordNet that has hypernyms (is-a) relationships.

Problems with this discrete representation:
Missing nuances
Missing new words
Subjective
Requires human labor to create and adapt
Hard to compute accuracy word similarity

Distributional similarity based representations:
Representing a word by means of its neighbors.

How to make neighbors represent words?
With a cooccurrence matrix X
a. Full document: Matrix X will give general topics leading to "Latent Semantic Analysis".
b. Window: Matrix captures syntactic (POS) and semantic information.

Window size: 1
Each word has its own matrix representation.
Matrix consists of count of neighboring words in the window size.

Problems with simple cooccurrence vectors:
Increase in size with vocabulary
Very high dimensional
Classification models have sparsity issues.
Models are less robust.

Solution: Low dimensional vectors

Method 1: Dimensionality Reduction on X
Singular Value Decomposition

Problems with SVD:
Computational cost
Bad for millions of words or documents
Hard to incorporate new words or documents

Idea: Directly learn low-dimensional word vectors
Old ideas:
Learning representations by back-propagating errors.
A neural probabilistic language model
word2vec

Main idea of word2vec:
Instead of capturing cooccurence counts directly, predict surrouding words of every word.
Faster and can easily incorporate a new sentence/document or add a word to the vocabulary.

Details of word2vec:
Predict surrounding words in a window of length m of every word.
Object function: Maximize the log probability of any context word given the current center word:
J(theta) = 1/T Sigma Sigma log p(wt+j|wt) where t = 1 to T and -m <= j <= m

For p(wt+j|wj), the simplest form is
p(o|c) = e^(u * v) / Sigma e^(u * v)

where
o: outside word id
c: center word id
u: center and outside word vector of o
v: center and outside word vector of c

Every word has two vector.
This is essentially "dynamic" logistic regression.

Objective/Cost functions:
Gradient descent

Approximations:
With large vocabularies, objective function is not scalable and would train too slowly.
Idea:
Approximate the normalization
Define negative prediction that only samples a few words that do not appear in the context.
Similar to focusing on mostly positive correlations.

Linear Relationships in word2vec:
Syntactically:
X(apple) - X(apples) = X(car) - X(cars) = X(family) - X(families)

Semantically:
X(shirt) - X(clothing) = X(chair) - X(furniture)

Count based vs direct prediction:
Count based: LSA, HAL, PCA
Fast training
Efficient usage of statistics
Primarily used to capture word similarity
Disproportionate importance given to large counts

Direct Prediction: RNN, Skip/gram, CBOW
Scales with corpus size
Inefficient usage of statistics
Generate improved performance on other tasks
Can capture complex patterns beyond word similarity

=================================================================================================================

Lecture 3
=========
Advanced word vector representations: language models, softmax, single layer networks

Word vectors:
Define the set of all parameters in a model in terms of one long vector theta

Gradient Descent:
To minimize J(theta) over the full batch would require us to compute gradients for all windows.

Updates for each element of theta:
theta_new = theta_old - alpha d J(theta)/d(theta_old)

Stochastic Gradient Descent:
We will update parameters after each window t.

Approximations: The skip-gram model
Main idea: train binary logistic regressions for a true pair (center word and word in its context window) and a couple of random pairs.

Continous bag of words model:
Main idea: Predict center word from sum of surrounding word vectors.

GloVe:
J(theta) = 1/2 Sigma f(Pij) (ui * vj - log Pij)^2

Good performance with small corpora.

What to do with the two sets of vectors?
U and V captures similar co-occurrence information. 
Xfinal = U + V

Evaluate word vectors:

a. Intrinsic:
Evaluation on a specific/intermediate subtasks
Fast to compute
Helps to understand the system

For e.g
Word vector analogies
a:b :: c:?
Human judgements

b. Extrinsic:
Evaluation on a real task
Can take a long time to compute accuracy.

For e.g
Name-entity recognition

Hyperparameters:
Best dimensions: 300
Window size: 8 is good for GloVe
Wikipedia corpus is better for training, but common crawl is better.

Handling ambiguity:
Idea: Cluster word windows around words, restrain with each word assigned to multiple different clusters bank1, bank2, etc.

Logistic regression: Softmax classification on word vector x to obtain probabilities for class y.
Loss function: Cross entropy

=======================================================================================================================
